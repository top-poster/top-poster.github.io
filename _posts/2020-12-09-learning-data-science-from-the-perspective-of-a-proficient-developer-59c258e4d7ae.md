---
layout: post
title: "숙련된 개발자의 관점에서 본 데이터 과학 학습"
author: "Logger"
thumbnail: "undefined"
tags: 
---


![child sitting in front of laptop raising their arms in a gesture of success or sudden understanding](https://miro.medium.com/max/16062/0*FT7FJrW7e_p6PoFT)

아시다시피, 데이터 과학, 특히 기계 학습이 현재 매우 유행하고 있습니다. 맞혀보세요? 나는 데이터 과학에 능숙해지기 위해 MOOC에 등록하기로 결심했다. 그러나 강력한 개발 배경에서 시작할 때는 프로그래밍의 주요 경험 중 하나일 때와 상당히 다릅니다. 그 이유는 이렇다.

# Dreyfus 기술 습득 모델

Dreyfus 모델은 학습자가 상황을 어떻게 인지하고, 어떤 요소에 초점을 맞추고, 결과에 얼마나 관여하고, 문제를 어떻게 다루는지 정의하는데 있어서 다른 단계를 거친다. 단순함을 위해 개발자가 모델에 따라 보여줄 수 있는 세 가지 얼굴, 즉 초보자(초보자), 숙련자(유능자), 전문가(일명 고수)에 초점을 맞추겠다.

간단히 말해서, 다른 단계는 다른 요구를 의미합니다. 프로그래밍은 데이터 과학을 위한 도구로 볼 수 있고, 데이터 과학 그 자체는 볼 수 없지만, 여러분의 일상 생활에서 여러분을 가장 많이 차지하는 것이 될 수 있습니다(이론자일 경우 제외). 그 결과, 프로그래밍의 실용적 기술을 익혔을 때 초보자로서의 데이터 과학을 배우는 것은 그리 효율적이지 않을 것이다. 앞으로 일어날 일이 아닌 일에 집중하는 것이 좋다.

# 파이톤 이즈 킹

전 세계 모든 데이터 과학자가 파이썬을 사용하고 있는 것처럼 보이므로 먼저 파이썬 프로그래밍을 배워야 한다고 생각할 수 있습니다. 이에 못지 않은 사실이 있다. 실제로 파이썬 전문가가 될 필요는 없습니다. 기본 사항만 알면 이미 다른 언어와의 유사점을 알 수 있습니다. 모듈을 가져오고, 포맷된 데이터를 인쇄하고, DREY 원칙을 따르려면 기능을 만들고, 루프 또는 분기를 다루십시오. 객체 지향 프로그래밍이나 설계 패턴을 다룰 필요는 거의 없습니다.

만약 당신이 숙련된 개발자이고 데이터 과학을 마스터하고 싶다면, 파이썬을 먼저 깊이 있게 배우는 것은 아마도 시간 낭비일 것이다.

Python이 King이라면, 궁정에서는 실제로 데이터 과학에 대한 에코시스템을 중점적으로 다루라고 권하고 싶습니다. 특히 다음과 같습니다.

생태계에 능숙해지고 싶다면 빨리 이해해야 할 가장 중요한 것 중 하나는 모든 데이터 과학 툴킷 사이에 데이터를 전달하는 방법이다. 간단히 말해서, 판다는 데이터 프레임이라고 불리는 피벗 형식을 제공합니다. 기본적으로 데이터 프레임은 스프레드시트(즉, 데이터 유형이 다를 수 있는 명명된 열)와 동일하지만 초능력(즉, 정교한 선택과 중추 메커니즘)이 있다. NumPy, Matplotlib 및 SciPy는 기본 배열을 입력으로 사용하거나 투명하게 캡슐화하는 낮은 수준의 툴킷입니다. 대부분의 경우 이러한 툴킷을 사용하려면 데이터 프레임에서 영상 시리즈만 추출하면 됩니다. 모델링 도구 키트는 더 높은 수준으로 일반적으로 데이터 프레임을 입력으로 직접 관리합니다. 또한 시각화를 위해 Altair 또는 Joypy와 같이 데이터 프레임을 입력으로 직접 관리하는 툴킷도 볼 수 있습니다.

만약 여러분이 숙련된 개발자이고 데이터 과학을 마스터하고 싶다면, 판다의 데이터 프레임으로 효율적으로 작업하는 방법을 배우는 것은 아마도 그만한 가치가 있을 것이다.

GUI 기반 툴킷을 사용하여 데이터 프레임 콘텐츠를 먼저 검색하고 분석함으로써 시간을 절약할 수 있습니다. 또한 판다의 IO 도구를 사용하여 원시 또는 처리된 데이터 프레임을 저장하여 처리 성능을 크게 절약할 수 있습니다. 실제로 대부분의 노트북은 CSV 데이터 파일을 읽는 방식으로 작동하므로 데이터 유지에 매우 비효율적이지만 훨씬 많은 옵션이 있습니다.

아나콘다는 여러 데이터 과학 패키지와 함께 제공되며 다양한 버전을 처리할 때 문제를 줄일 수 있도록 여러 환경을 관리할 수 있기 때문에 데이터 작업을 시작할 준비가 되어 있는 경우가 많습니다. 그러나 아나콘다는 200개 이상의 패키지를 가진 상당히 큰 소프트웨어이므로 업데이트할 때 때때로 몇 시간 동안 기다려야 할 수도 있습니다. 초보자들보다는 패키지 관리(의존성 지옥)에 더 정통한 여러분은 아마 가장 유용한 구성 요소만을 포함하는 아나콘다의 작은 부트스트랩 버전인 미니콘다를 선호할 것이다.

# 정규 분포는 퀸입니다.

모든 데이터 과학자가 정규 분포를 사용하는 것으로 보이므로 가우스 분포를 먼저 배워야 한다고 생각할 수 있습니다. 더 이상 진실된 것은 없다. 실제로, 중심 한계 정리는 충분히 큰 표본 크기(통계학자가 일반적으로 권장하는 30)에 대해 변수에 대한 평균의 표본 분포는 모집단 분포와 관계없이 정규 분포를 따른다고 명시한다. 따라서 상관 관계, 회귀 분석, t-검정, 특이치 탐지 및 분산 분석을 포함한 많은 통계 방법은 관심 있는 값이 정규 분포를 따른다고 가정합니다. 방법에 따라 데이터가 항상 정규 분포를 따르는 것이 아니라 데이터의 평균 또는 오차 잔차를 의미하는 것이 되도록 주의하십시오.

그러나 입력 데이터가 이를 분석하는 데 사용된 통계 방법의 가정을 충족하는지 확인하지 않는 것은 연구에서 발견되는 가장 일반적인 오류 중 하나이다.

초보자로서 데이터에 방법(문맥 인식)을 적용하기 전에 항상 육안 검사(예: 히스토그램 및 Q-Q 그림) 및/또는 유의성 검정(예: 샤피로-윌크 검정)을 사용하여 정규성을 확인하라는 지시를 받는 것은 아닙니다. 결과를 해석하기 전에 항상 데이터를 보는 것이 최소한의 방법입니다.

기계 학습은 실제로 새로운 상황에 대한 일반화입니다. 특정 데이터 집합에만 존재하는 패턴은 무의미하며, 원하는 패턴은 그 너머에도 존재합니다. 분포는 바로 이러한 패턴이며, 일반적으로 사용되는 평균 값을 연구한다면 정규 분포는 퀸입니다.

# 생산 격차 현실화

다른 연구 활동과 마찬가지로, 데이터 과학의 많은 부분은 모델 엔지니어링, 하이퍼 파라미터 최적화, 모델 테스트 등 실험에 전념한다. 이는 교사들이 기술적인 문제를 피하고 개념에 초점을 맞추기 위해 완전히 통제된 환경(일반적으로 랩톱)을 선호하기 때문에 학습 시 더욱 악화됩니다.

그러나 숙련된 개발자로서 여러분은 자연스럽게 생산 제약과 탐색적 코드와 생산 준비 코드 간의 차이를 알게 됩니다. 한편, 클라우드를 애플리케이션의 주요 인프라로 채택하는 것은 노트북에 문제를 제기하며, 노트북에서 로컬로 실행할 경우 클라이언트-서버 아키텍처에 대한 추론이 모호해집니다. 반면에 실제 데이터는 거대할 수 있으며 노트북에 비해 너무 많은 컴퓨팅 집약적인 모델입니다. 프로덕션과 유사한 환경에서는 데이터를 탐색하고 기술을 개선할 수 있는 더 많은 기회를 제공할 수 있습니다. 마지막으로 중요한 것은 탐사가 오류 발생 가능성이 매우 높기 때문에 이전 버전의 작업을 쉽게 복원할 수 있어야 하며 팀 내에서 동시에 작업할 수도 있어야 한다는 점입니다.

따라서, 다음과 같은 접근 방식을 조기에 채택하여 운영 간 탐색 격차를 크게 줄여야 합니다.

## 1. 개발 워크플로우 설정

GitHub은 개발을 위한 버전 제어 시스템의 사실상의 표준이 되었지만, 데이터 과학은 직접적으로 다룰 수 없는 특정한 문제들을 보여준다. 첫째, 소프트웨어 프로젝트 구조는 실제로 관련이 없습니다. Cookiecutter Data Science와 같은 것이 아마도 더 나은 선택이거나 적어도 영감의 원천일 것입니다. 둘째, 대용량 데이터 파일 및 노트북을 관리해야 합니다. 이러한 파일은 Git에 적합하지 않습니다(Diff/Merge 작업은 사람이 읽을 수 없습니다). DVC(Data Version Control)가 AWS S3 또는 Google Drive와 같은 클라우드 기반 서비스에서 데이터 세트 버전 관리 및 스토리지를 관리할 수 있기를 바랍니다. 또한 nbdime/nbspripout은 노트북에서 Git 작업을 원활하게 유지할 수 있도록 도와줍니다.

## 2. 작업 구성 가능

데이터에 적합한 모델을 찾기 위해 일부 매개 변수를 사용해야 하는 경우가 많지만 노트북에서 하드코딩 값을 변경하는 것이 가장 효율적인 방법은 아닙니다. papermill은 구성 가능한 명령줄 스크립트를 실행하는 것처럼 일부 입력 매개 변수에 따라 노트북을 새 노트북으로 래핑합니다. Streamlight를 사용하면 각 매개 변수를 완전한 대화형 웹 앱의 위젯으로 만들 수 있습니다. 그러나 먼저 nbconvert를 사용하여 노트를 Python 스크립트로 내보내야 합니다.

## 3. 작업을 디버깅할 수 있게 합니다.

디버깅하기 위해 우리 코드에 인쇄() 문을 추가하는 것은 아마도 개발자들 사이에서 가장 일반적인 관행 중 하나일 것이다. 하지만 앞서 말씀드린 바와 같이 보다 정교한 솔루션을 이용할 수 있습니다.

로깅은 매우 강력한 도구이지만 작업에 적합한 도구를 사용하는 것이 중요합니다. 첫째, 다양한 심각도 수준(예: 개발 정보, 프로덕션 오류 메시지 등)에 따라 로그를 필터링/활성화할 수 있어야 합니다. 둘째, 상황 정보가 누락되지 않도록 필요할 때마다 개체를 자세히 검사하고 추적을 쌓을 수 있어야 합니다. 셋째, 화면 대신 또는 화면에 로그를 기록할 수도 있습니다. 마지막으로 중요한 것은 대규모 프로젝트에 대한 너무 많은 정보를 제공하지 않도록 서로 다른 도메인(예: 데이터 사전 처리, 모델 장착 등)에 따라 필요에 따라 로그를 필터링할 수 있어야 한다는 것입니다. 예를 들어, 내장 로깅 모듈이 이미 풍부한 기능 세트를 제공하지만, Python에는 로구루가 매우 좋은 선택이 될 수 있다.

물론 가능한 경우(예: 비 프로덕션 환경) 디버거를 사용할 수 있는 최상의 옵션이 남아 있습니다. 다른 프로그래밍 언어와 마찬가지로 파이썬도 디버거가 내장되어 있습니다. 이 무기를 효과적으로 사용하는 방법을 배우면 됩니다. 중단점을 설정하거나 단일 단계 실행을 보다 편안하게 사용하려면 Visual Studio Code 또는 PyCharm과 같은 대화형 디버깅을 지원하는 IDE(통합 개발 환경)가 필요합니다.

## 4. 데이터 파이프라인으로 작업 가능

노트북은 기본적으로 코드, 시각화 및 텍스트를 포함하는 대화형 웹 인터페이스입니다. 실험이나 데이터 분석에 매우 유용하지만, CI 파이프라인을 통해 자동으로 실행되거나 처리되도록 설계되지는 않았습니다. 어느 정도 생산 시 노트북을 사용하는 것은 안티패턴으로도 볼 수 있다.

작업을 실행 가능한 파이프라인으로 변환하는 가장 간단한 방법은 nbconvert를 사용하여 노트를 파이썬 스크립트로 내보내거나 페이퍼밀을 통해 실행하는 것입니다. 그러나 어떤 것도 스크립트를 노트북으로 가져올 수 없기 때문에 스크립트를 미리 사용하는 것이 더 좋은 방법일 수 있습니다(그냥 파이썬 코드죠?). 이렇게 하면 비 대화식 프로세스에서도 호출할 수 있습니다. 예를 들어, DVC(우리는 이미 그것을 충족시켰습니다)는 최종 결과를 생성하는 일련의 데이터 프로세스인 데이터 파이프라인을 캡처하는 메커니즘을 제공합니다.

## 5. 관리형 서비스에서 제공하는 기회 활용

데이터가 커지고 모델이 복잡해짐에 따라 머신 러닝 작업은 노트북에 비해 컴퓨팅 집약도가 너무 높아지기 시작합니다. 그런 다음 클라우드 서비스에 워크로드를 위임(일부)할 가치가 있는 경우가 많습니다. Kaggle, Google 협업, AI 플랫폼 노트북, Amazon SageMaker, FloydHub, SaturnCloud, CoCalc 등이 이를 위해 가장 잘 알려진 서비스 중 하나일 것이다. 새로운 경향은 OVH 또는 TADA와 같은 SaaS 노트북보다 머신 러닝을 위한 더 많은 통합 플랫폼을 제공하는 것이다.

# 간단한 기계 학습

여러분은 제가 왜 지금까지 기계 학습 알고리즘에 대해 말하지 않았는지 궁금하실 것입니다. 데이터 과학에서 가장 어렵고 시간이 많이 소요되는 부분은 모델 엔지니어링(즉, 데이터를 유용하게 만드는 것)이 아니라 실제로 데이터 엔지니어링(즉, 데이터를 사용할 수 있게 만드는 것)인 것으로 보인다. 모델을 구축하는 것은 실제로 머신 러닝 툴킷의 도움으로 매우 간단하며, 프로세스는 거의 표준화되어 있다.

툴킷은 또한 모델 엔지니어링, 하이퍼 파라미터 최적화 및 모델 테스트를 돕거나 자동화하는 기능을 제공합니다. 그래서 기계 학습 툴킷에 의해 제공되는 "블랙박스"를 이용한 모델링은 여러분이 능숙한 프로그래머일 때 실제로 매우 간단합니다. 저의 겸손한 생각으로는, 숙련된 개발자로서, 가장 중요한 것 중 하나는 기계 학습의 한계입니다. 실제로 귀하의 경험 덕분에 우수한 데이터와 모델 엔지니어링을 신속하게 수행할 수 있을 것입니다. 하지만, 여러분은 몇 가지를 기억해야 합니다.

첫째, 머신 러닝은 데이터 없이는 아무것도 아니지만, 데이터가 사용 사례에 대한 실제 효과를 모두 캡처할 수 있을 만큼 충분한지 알 수 없을 것입니다. 분명히 기계 학습은 입력 데이터가 부족한 문제에 적용될 수 없다. 그러나 많은 양의 데이터를 사용할 수 있는 경우에도 데이터는 노이즈가 발생할 수 있으며 오류나 편향을 포함할 수 있습니다.

둘째, 기계 학습은 일반적으로 결정론적 프로세스가 아닌 확률론적 프로세스(특히 딥 러닝)에 기초한다. 일반적으로 모델은 물리적 법칙을 이해하거나 물리적 제약을 포함하지 않으며 비현실적인 값을 예측할 수 있다. 요컨대 우리 자신의 판단보다 알고리즘을 더 신뢰하면 문제가 발생하고 설명 능력이 떨어진다.

마지막으로 중요한 것은 기계 학습이 입력 데이터와 모델의 결과(예: 예측) 사이의 상관 관계를 밝히는 것을 목표로 한다는 점이다. 인간의 뇌는 패턴이 존재하지 않을 때도 패턴을 찾으려고 하기 때문에 상관관계는 인과관계와 혼동되는 경우가 많다. 그러나 상관관계가 인과관계를 의미하는 것은 아니다. 원인은 예측에 영향을 미치는 실제 요인을 증명하는 것이고, 모형이 종종 "충분히 좋은" 바로 가기를 사용하는 것입니다. 예를 들어, 실제 생활에서 A에서 D로 이어지는 경로는 D에 도달하기 전에 B와 C를 통과하는 반면, 모델은 A와 D 사이에 직접적인 관계를 형성할 수 있습니다.

요즘 데이터 과학 및 머신러닝 생태계가 빠르게 움직이고 있는 만큼, 파이썬을 넘어 수소, 중성, 폴리노트로 새로운 실험 방법을 모색해 보자는 제안으로 마무리하고 싶다.